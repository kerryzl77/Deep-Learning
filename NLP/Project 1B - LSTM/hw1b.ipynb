{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Project 1b: Language Modeling\n","\n","In this project, you will implement several different types of language models for text.  We'll start with n-gram models, then move on to neural n-gram and LSTM language models.\n","\n","Warning: Do not start this project the day before it is due!  Some parts require 20 minutes or more to run, so debugging and tuning can take a significant amount of time.\n","\n","Our dataset for this project will be the Penn Treebank language modeling dataset.  This dataset comes with some of the basic preprocessing done for us, such as tokenization and rare word filtering (using the `<unk>` token).\n","Therefore, we can assume that all word types in the test set also appear at least once in the training set.\n","We'll also use the `torchtext` library to help with some of the data preprocessing, such as converting tokens into id numbers."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-13T16:08:32.347207Z","iopub.status.busy":"2024-09-13T16:08:32.346859Z","iopub.status.idle":"2024-09-13T16:09:35.758450Z","shell.execute_reply":"2024-09-13T16:09:35.757106Z","shell.execute_reply.started":"2024-09-13T16:08:32.347130Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torchtext==0.10.0\n","  Downloading torchtext-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.10.0) (2.28.1)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.10.0) (4.64.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext==0.10.0) (1.21.6)\n","Collecting torch==1.9.0\n","  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.4/831.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.9.0->torchtext==0.10.0) (4.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.10.0) (3.3)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.10.0) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.10.0) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.10.0) (1.26.14)\n","Installing collected packages: torch, torchtext\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.11.0\n","    Uninstalling torch-1.11.0:\n","      Successfully uninstalled torch-1.11.0\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.12.0\n","    Uninstalling torchtext-0.12.0:\n","      Successfully uninstalled torchtext-0.12.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pytorch-lightning 1.9.0 requires torch>=1.10.0, but you have torch 1.9.0 which is incompatible.\n","allennlp 2.10.1 requires torch<1.13.0,>=1.10.0, but you have torch 1.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-1.9.0 torchtext-0.10.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["# Some of the functions below require an older version of torchtext than the default one Kaggle gives you.\n","# IMPORTANT: Make sure that Internet is turned on!!! (Notebook options in the bar on the right)\n","# IMPORTANT: If you're not already using Kaggle, we STRONGLY recommend you switch to Kaggle for hw1b in particular,\n","# because copying our notebook will pin you to a Python version that lets you install the right version of torchtext.\n","# On Colab you will have to downgrade your Python to e.g., 3.7 to do the below pip install, which is a pain to do.\n","!pip install torchtext==0.10.0\n","exit()"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-13T16:09:35.761910Z","iopub.status.busy":"2024-09-13T16:09:35.761391Z","iopub.status.idle":"2024-09-13T16:09:35.881792Z","shell.execute_reply":"2024-09-13T16:09:35.880811Z","shell.execute_reply.started":"2024-09-13T16:09:35.761851Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'/kaggle/working/wikitext-2'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Copy wikitext-2 dataset folder to /kaggle/working\n","import shutil\n","shutil.copytree('/kaggle/input/wikitext-2', '/kaggle/working/wikitext-2')"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-13T16:12:00.227477Z","iopub.status.busy":"2024-09-13T16:12:00.227074Z","iopub.status.idle":"2024-09-13T16:12:01.587690Z","shell.execute_reply":"2024-09-13T16:12:01.586697Z","shell.execute_reply.started":"2024-09-13T16:12:00.227441Z"},"executionInfo":{"elapsed":1116,"status":"ok","timestamp":1614318181024,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":480},"id":"X1ATX-_J7SjQ","outputId":"22d9213d-2b3b-415a-960c-bbbe88533690","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['<eos>', '=', 'Homarus', 'gammarus', '=', '<eos>', '<eos>', 'Homarus', 'gammarus', ',', 'known', 'as', 'the', 'European', 'lobster', 'or', 'common', 'lobster', ',', 'is', 'a', 'species', 'of', '<unk>', 'lobster', 'from', 'the', 'eastern', 'Atlantic', 'Ocean']\n"]}],"source":["# This block handles some basic setup and data loading.  \n","# You shouldn't need to edit this, but if you want to \n","# import other standard python packages, that is fine.\n","\n","# imports\n","from collections import defaultdict, Counter\n","import numpy as np\n","import math\n","import tqdm\n","import random\n","import pdb\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torchtext\n","import torch.optim as optim\n","\n","from torchtext.legacy import data\n","from torchtext.legacy import datasets\n","\n","# download and load the data\n","text_field = data.Field()\n","datasets = datasets.WikiText2.splits(root='.', text_field=text_field)\n","train_dataset, validation_dataset, test_dataset = datasets\n","\n","text_field.build_vocab(train_dataset, validation_dataset, test_dataset)\n","vocab = text_field.vocab\n","vocab_size = len(vocab)\n","\n","train_text = train_dataset.examples[0].text # a list of tokens (strings)\n","validation_text = validation_dataset.examples[0].text\n","\n","print(validation_text[:30])"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-13T16:12:03.937242Z","iopub.status.busy":"2024-09-13T16:12:03.936710Z","iopub.status.idle":"2024-09-13T16:12:03.946615Z","shell.execute_reply":"2024-09-13T16:12:03.945618Z","shell.execute_reply.started":"2024-09-13T16:12:03.937207Z"},"id":"QoFMSckDFTop","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['<eos>', '=', 'Homarus', 'gammarus', '=', '<eos>', '<eos>', 'Homarus', 'gammarus', ',', 'known', 'as', 'the', 'European', 'lobster', 'or', 'common', 'lobster', ',', 'is', 'a', 'species', 'of', '<unk>', 'lobster', 'from', 'the', 'eastern', 'Atlantic', 'Ocean', ',', 'Mediterranean', 'Sea', 'and', 'parts', 'of', 'the', 'Black', 'Sea', '.', 'It', 'is', 'closely', 'related', 'to', 'the', 'American', 'lobster', ',', 'H.', 'americanus', '.', 'It', 'may', 'grow', 'to', 'a', 'length', 'of', '60', 'cm', '(', '24', 'in', ')', 'and', 'a', 'mass', 'of', '6', 'kilograms', '(', '13', 'lb', ')', ',', 'and', 'bears', 'a', 'conspicuous', 'pair', 'of', 'claws', '.', 'In', 'life', ',', 'the', 'lobsters', 'are', 'blue', ',', 'only', 'becoming', '\"', 'lobster', 'red', '\"', 'on', 'cooking', '.', 'Mating', 'occurs', 'in', 'the', 'summer', ',', 'producing', 'eggs', 'which', 'are', 'carried', 'by', 'the', 'females', 'for', 'up', 'to', 'a', 'year', 'before', 'hatching', 'into', '<unk>', 'larvae', '.', 'Homarus', 'gammarus', 'is', 'a', 'highly', 'esteemed', 'food', ',', 'and', 'is', 'widely', 'caught', 'using', 'lobster', 'pots', ',', 'mostly', 'around', 'the', 'British', 'Isles', '.', '<eos>', '<eos>', '=', '=', 'Description', '=', '=', '<eos>', '<eos>', 'Homarus', 'gammarus', 'is', 'a', 'large', '<unk>', ',', 'with', 'a', 'body', 'length', 'up', 'to', '60', 'centimetres', '(', '24', 'in', ')', 'and', 'weighing', 'up', 'to', '5', '–', '6', 'kilograms', '(', '11', '–', '13', 'lb', ')', ',', 'although', 'the', 'lobsters', 'caught', 'in', 'lobster', 'pots', 'are', 'usually', '23', '–', '38', 'cm', '(', '9', '–', '15', 'in', ')', 'long', 'and', 'weigh', '0', '@.@', '7', '–', '2', '@.@', '2', 'kg', '(', '1', '@.@', '5', '–', '4', '@.@', '9', 'lb', ')', '.', 'Like', 'other', 'crustaceans', ',', 'lobsters', 'have', 'a', 'hard', '<unk>', 'which', 'they', 'must', 'shed', 'in', 'order', 'to', 'grow', ',', 'in', 'a', 'process', 'called', '<unk>', '(', '<unk>', ')', '.', 'This', 'may', 'occur', 'several', 'times', 'a', 'year', 'for', 'young', 'lobsters', ',', 'but', 'decreases', 'to', 'once', 'every', '1', '–', '2', 'years', 'for', 'larger', 'animals', '.', '<eos>', 'The', 'first', 'pair', 'of', '<unk>', 'is', 'armed', 'with', 'a', 'large', ',', 'asymmetrical', 'pair', 'of', 'claws', '.']\n"]},{"data":{"text/plain":["90077"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["print(validation_text[:300])\n","text_field.vocab.freqs['.']"]},{"cell_type":"markdown","metadata":{"id":"g10PLGiZn0XY"},"source":["We've implemented a unigram model here as a demonstration."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-13T16:45:37.189366Z","iopub.status.busy":"2024-09-13T16:45:37.188988Z","iopub.status.idle":"2024-09-13T16:45:37.813225Z","shell.execute_reply":"2024-09-13T16:45:37.812258Z","shell.execute_reply.started":"2024-09-13T16:45:37.189333Z"},"executionInfo":{"elapsed":922,"status":"ok","timestamp":1614318184013,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":480},"id":"B7ZHMVZzoPEH","outputId":"475b17e5-ab16-46a4-93ec-5b9a44b4874c","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["unigram validation perplexity: 965.0860734119312\n"]}],"source":["class UnigramModel:\n","    def __init__(self, train_text):\n","        self.counts = Counter(train_text)\n","        self.total_count = len(train_text)\n","\n","    def probability(self, word):\n","        return self.counts[word] / self.total_count\n","\n","    def next_word_probabilities(self, text_prefix):\n","        \"\"\"Return a list of probabilities for each word in the vocabulary.\"\"\"\n","        return [self.probability(word) for word in vocab.itos]\n","\n","    def perplexity(self, full_text):\n","        \"\"\"Return the perplexity of the model on a text as a float.\n","        \n","        full_text -- a list of string tokens\n","        \"\"\"\n","        log_probabilities = []\n","        for word in full_text:\n","            # Note that the base of the log doesn't matter \n","            # as long as the log and exp use the same base.\n","            log_probabilities.append(math.log(self.probability(word), 2))\n","        return 2 ** -np.mean(log_probabilities)\n","\n","unigram_demonstration_model = UnigramModel(train_text)\n","print('unigram validation perplexity:', \n","      unigram_demonstration_model.perplexity(validation_text))\n","\n","def check_validity(model):\n","    \"\"\"Performs several sanity checks on your model:\n","    1) That next_word_probabilities returns a valid distribution\n","    2) That perplexity matches a perplexity calculated from next_word_probabilities\n","\n","    Although it is possible to calculate perplexity from next_word_probabilities, \n","    it is still good to have a separate more efficient method that only computes \n","    the probabilities of observed words.\n","    \"\"\"\n","\n","    log_probabilities = []\n","    for i in range(10):\n","        prefix = validation_text[:i]\n","        probs = model.next_word_probabilities(prefix)\n","        assert min(probs) >= 0, \"Negative value in next_word_probabilities\"\n","        assert max(probs) <= 1 + 1e-8, \"Value larger than 1 in next_word_probabilities\"\n","        assert abs(sum(probs)-1) < 1e-4, \"next_word_probabilities do not sum to 1\"\n","\n","        word_id = vocab.stoi[validation_text[i]]\n","        selected_prob = probs[word_id]\n","        log_probabilities.append(math.log(selected_prob))\n","\n","    perplexity = math.exp(-np.mean(log_probabilities))\n","    your_perplexity = model.perplexity(validation_text[:10])\n","    assert abs(perplexity-your_perplexity) < 0.1, \"your perplexity does not \" + \\\n","    \"match the one we calculated from `next_word_probabilities`,\\n\" + \\\n","    \"at least one of `perplexity` or `next_word_probabilities` is incorrect.\\n\" + \\\n","    f\"we calcuated {perplexity} from `next_word_probabilities`,\\n\" + \\\n","    f\"but your perplexity function returned {your_perplexity} (on a small sample).\"\n","\n","\n","check_validity(unigram_demonstration_model)"]},{"cell_type":"markdown","metadata":{"id":"U4esz5XrEpNo"},"source":["To generate from a language model, we can sample one word at a time conditioning on the words we have generated so far."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-13T16:45:39.305259Z","iopub.status.busy":"2024-09-13T16:45:39.304870Z","iopub.status.idle":"2024-09-13T16:45:39.638726Z","shell.execute_reply":"2024-09-13T16:45:39.637765Z","shell.execute_reply.started":"2024-09-13T16:45:39.305227Z"},"id":"bfNj5nl4E7Zn","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<eos> <eos> last balance Fear : 1998 At where <unk> latter Official video . army April Visions included <eos> UEFA confronted Gielgud\n"]}],"source":["def generate_text(model, n=20, prefix=('<eos>', '<eos>')):\n","    prefix = list(prefix)\n","    for _ in range(n):\n","        probs = model.next_word_probabilities(prefix)\n","        word = random.choices(vocab.itos, probs)[0]\n","        prefix.append(word)\n","    return ' '.join(prefix)\n","\n","print(generate_text(unigram_demonstration_model))"]},{"cell_type":"markdown","metadata":{"id":"wq-WtaM6F6kN"},"source":["In fact there are many strategies to get better-sounding samples, such as only sampling from the top-k words or sharpening the distribution with a temperature.  You can read more about sampling from a language model in this recent paper: https://arxiv.org/pdf/1904.09751.pdf."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T00:55:15.521754Z","iopub.status.busy":"2024-09-08T00:55:15.521405Z","iopub.status.idle":"2024-09-08T00:55:16.209090Z","shell.execute_reply":"2024-09-08T00:55:16.207770Z","shell.execute_reply.started":"2024-09-08T00:55:15.521725Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<eos> <eos> U2 by episode two The against around and @-@ erosion for = a SR result predators The that . and\n"]}],"source":["import numpy as np\n","import random\n","\n","def nucleus_sampling(probs, p=0.95):\n","    sorted_indices = np.argsort(probs)[::-1]\n","    cumulative_probs = np.cumsum(np.sort(probs)[::-1])\n","    cutoff_index = np.searchsorted(cumulative_probs, p)\n","    nucleus_indices = sorted_indices[:cutoff_index + 1]\n","    nucleus_probs = np.array([probs[i] for i in nucleus_indices])\n","    nucleus_probs /= np.sum(nucleus_probs)  # Normalize\n","    return nucleus_indices, nucleus_probs  # Return both the indices and the corresponding probabilities\n","\n","def generate_text_with_nucleus_sampling(model, n=20, prefix=('<eos>', '<eos>')):\n","    prefix = list(prefix)\n","    for _ in range(n):\n","        probs = model.next_word_probabilities(prefix)\n","        nucleus_indices, nucleus_probs = nucleus_sampling(probs, p=0.95)\n","        selected_index = random.choices(nucleus_indices, nucleus_probs)[0]\n","        word = vocab.itos[selected_index]\n","        prefix.append(word)\n","    return ' '.join(prefix)\n","\n","# Assuming vocab and model are properly defined\n","print(generate_text_with_nucleus_sampling(unigram_demonstration_model))\n"]},{"cell_type":"markdown","metadata":{"id":"uuopg4rYjf2O"},"source":["You will need to submit some outputs from the models you implement for us to grade.  The following function will be used to generate the required output files."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-13T16:12:09.637668Z","iopub.status.busy":"2024-09-13T16:12:09.636790Z","iopub.status.idle":"2024-09-13T16:12:15.562014Z","shell.execute_reply":"2024-09-13T16:12:15.560842Z","shell.execute_reply.started":"2024-09-13T16:12:09.637632Z"},"executionInfo":{"elapsed":830,"status":"ok","timestamp":1614318186687,"user":{"displayName":"Rudy Corona Rodriguez","photoUrl":"","userId":"02448394073714905143"},"user_tz":480},"id":"ZB6MKPbm4z9s","outputId":"3b7db928-270a-4150-f40b-ba7dadbc7d8e","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-09-13 16:12:10--  https://cal-cs288.github.io/sp21/project_files/proj_1/eval_prefixes.txt\n","Resolving cal-cs288.github.io (cal-cs288.github.io)... 185.199.111.153, 185.199.110.153, 185.199.108.153, ...\n","Connecting to cal-cs288.github.io (cal-cs288.github.io)|185.199.111.153|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 519055 (507K) [text/plain]\n","Saving to: ‘eval_prefixes.txt’\n","\n","eval_prefixes.txt   100%[===================>] 506.89K  3.17MB/s    in 0.2s    \n","\n","2024-09-13 16:12:11 (3.17 MB/s) - ‘eval_prefixes.txt’ saved [519055/519055]\n","\n","--2024-09-13 16:12:12--  https://cal-cs288.github.io/sp21/project_files/proj_1/eval_output_vocab.txt\n","Resolving cal-cs288.github.io (cal-cs288.github.io)... 185.199.110.153, 185.199.109.153, 185.199.108.153, ...\n","Connecting to cal-cs288.github.io (cal-cs288.github.io)|185.199.110.153|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 12497 (12K) [text/plain]\n","Saving to: ‘eval_output_vocab.txt’\n","\n","eval_output_vocab.t 100%[===================>]  12.20K  --.-KB/s    in 0.001s  \n","\n","2024-09-13 16:12:12 (12.3 MB/s) - ‘eval_output_vocab.txt’ saved [12497/12497]\n","\n","--2024-09-13 16:12:13--  https://cal-cs288.github.io/sp21/project_files/proj_1/eval_prefixes_short.txt\n","Resolving cal-cs288.github.io (cal-cs288.github.io)... 185.199.108.153, 185.199.110.153, 185.199.111.153, ...\n","Connecting to cal-cs288.github.io (cal-cs288.github.io)|185.199.108.153|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 105976 (103K) [text/plain]\n","Saving to: ‘eval_prefixes_short.txt’\n","\n","eval_prefixes_short 100%[===================>] 103.49K  --.-KB/s    in 0.08s   \n","\n","2024-09-13 16:12:14 (1.25 MB/s) - ‘eval_prefixes_short.txt’ saved [105976/105976]\n","\n","--2024-09-13 16:12:15--  https://cal-cs288.github.io/sp21/project_files/proj_1/eval_output_vocab_short.txt\n","Resolving cal-cs288.github.io (cal-cs288.github.io)... 185.199.108.153, 185.199.109.153, 185.199.111.153, ...\n","Connecting to cal-cs288.github.io (cal-cs288.github.io)|185.199.108.153|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3802 (3.7K) [text/plain]\n","Saving to: ‘eval_output_vocab_short.txt’\n","\n","eval_output_vocab_s 100%[===================>]   3.71K  --.-KB/s    in 0s      \n","\n","2024-09-13 16:12:15 (47.7 MB/s) - ‘eval_output_vocab_short.txt’ saved [3802/3802]\n","\n"]}],"source":["!wget https://cal-cs288.github.io/sp21/project_files/proj_1/eval_prefixes.txt\n","!wget https://cal-cs288.github.io/sp21/project_files/proj_1/eval_output_vocab.txt\n","!wget https://cal-cs288.github.io/sp21/project_files/proj_1/eval_prefixes_short.txt\n","!wget https://cal-cs288.github.io/sp21/project_files/proj_1/eval_output_vocab_short.txt\n","\n","def save_truncated_distribution(model, filename, short=True):\n","    \"\"\"Generate a file of truncated distributions.\n","    \n","    Probability distributions over the full vocabulary are large,\n","    so we will truncate the distribution to a smaller vocabulary.\n","\n","    Please do not edit this function\n","    \"\"\"\n","    vocab_name = 'eval_output_vocab'\n","    prefixes_name = 'eval_prefixes'\n","\n","    if short: \n","      vocab_name += '_short'\n","      prefixes_name += '_short'\n","\n","    with open('{}.txt'.format(vocab_name), 'r') as eval_vocab_file:\n","        eval_vocab = [w.strip() for w in eval_vocab_file]\n","    eval_vocab_ids = [vocab.stoi[s] for s in eval_vocab]\n","\n","    all_selected_probabilities = []\n","    with open('{}.txt'.format(prefixes_name), 'r') as eval_prefixes_file:\n","        lines = eval_prefixes_file.readlines()\n","        for line in tqdm.tqdm_notebook(lines, leave=False):\n","        # Compatible Save with Trigram Backoff model\n","        # for line in tqdm(lines, leave=False):\n","            prefix = line.strip().split(' ')\n","            probs = model.next_word_probabilities(prefix)\n","            selected_probs = np.array([probs[i] for i in eval_vocab_ids], dtype=np.float32)\n","            all_selected_probabilities.append(selected_probs)\n","\n","    all_selected_probabilities = np.stack(all_selected_probabilities)\n","    np.save(filename, all_selected_probabilities)\n","    print('saved', filename)"]},{"cell_type":"markdown","metadata":{"id":"MEfUwbbS9vy0"},"source":["### N-gram Model\n","\n","Now it's time to implement an n-gram language model.\n","\n","Because not every n-gram will have been observed in training, use add-alpha smoothing to make sure no output word has probability 0.\n","\n","$$P(w_2|w_1)=\\frac{C(w_1,w_2)+\\alpha}{C(w_1)+N\\alpha}$$\n","\n","where $N$ is the vocab size and $C$ is the count for the given bigram.  An alpha value around `3e-3`  should work.  Later, we'll replace this smoothing with model backoff.\n","\n","One edge case you will need to handle is at the beginning of the text where you don't have `n-1` prior words.  You can handle this however you like as long as you produce a valid probability distribution, but just using a uniform distribution over the vocabulary is reasonable for the purposes of this project.\n","\n","A properly implemented bi-gram model should get a perplexity below 510 on the validation set.\n","\n","**Note**: Do not change the signature of the `next_word_probabilities` and `perplexity` functions.  We will use these as a common interface for all of the different model types.  Make sure these two functions call `n_gram_probability`, because later we are going to override `n_gram_probability` in a subclass. \n","Also, we suggest pre-computing and caching the counts $C$ when you initialize `NGramModel` for efficiency. "]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-03T22:12:03.784084Z","iopub.status.busy":"2024-09-03T22:12:03.783672Z","iopub.status.idle":"2024-09-03T22:12:19.314555Z","shell.execute_reply":"2024-09-03T22:12:19.313620Z","shell.execute_reply.started":"2024-09-03T22:12:03.784050Z"},"id":"_nzVrTWcH67Q","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:28: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["saved unigram_demonstration_predictions.npy\n"]}],"source":["save_truncated_distribution(unigram_demonstration_model, \n","                            'unigram_demonstration_predictions.npy')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T19:22:29.461307Z","iopub.status.busy":"2024-09-11T19:22:29.460900Z","iopub.status.idle":"2024-09-11T19:22:29.817821Z","shell.execute_reply":"2024-09-11T19:22:29.816686Z","shell.execute_reply.started":"2024-09-11T19:22:29.461275Z"},"id":"YGnGpnPIXpTW","trusted":true},"outputs":[],"source":["class NGramModel:\n","    def __init__(self, train_text, n=2, alpha=3e-3):\n","        # get counts and perform any other setup\n","        self.n = n\n","        self.smoothing = alpha\n","        self.train_text = train_text\n","\n","        # YOUR CODE HERE\n","\n","        # Create (n-1)-grams and n-grams counts\n","        if n > 1:\n","            self.n_gram_counts = Counter(tuple(train_text[i:i+n]) for i in range(len(train_text) - n + 1))\n","            self.n_minus_1_gram_counts = Counter(tuple(train_text[i:i+n-1]) for i in range(len(train_text) - n + 2))\n","        else:\n","            self.n_gram_counts = Counter(train_text)\n","            self.n_minus_1_gram_counts = Counter(train_text)  # For consistent smoothing\n","        \n","        self.vocab_size = len(set(train_text))\n","        self.total_count = len(train_text)\n","\n","    def n_gram_probability(self, n_gram):\n","        \"\"\"Return the probability of the last word in an n-gram.\n","        \n","        n_gram -- a list of string tokens\n","        returns the conditional probability of the last token given the rest.\n","        \"\"\"\n","        assert len(n_gram) == self.n\n","        \n","        # YOUR CODE HERE\n","        if self.n > 1:\n","            n_minus_1_gram_tuple = tuple(n_gram[:-1])\n","            n_gram_tuple = tuple(n_gram)\n","            n_gram_count = self.n_gram_counts.get(n_gram_tuple, 0)  # Handle unseen n-grams\n","            n_minus_1_gram_count = self.n_minus_1_gram_counts.get(n_minus_1_gram_tuple, 0)\n","            probability = (n_gram_count + self.smoothing) / (n_minus_1_gram_count + self.vocab_size * self.smoothing)\n","        else:\n","            # Apply smoothing for unigrams too\n","            n_gram_count = self.n_gram_counts.get(n_gram[0], 0)\n","            probability = (n_gram_count + self.smoothing) / (self.total_count + self.vocab_size * self.smoothing)\n","        return probability\n","        \n","        \n","\n","    def next_word_probabilities(self, text_prefix):\n","        \"\"\"Return a list of probabilities for each word in the vocabulary.\"\"\"\n","\n","        # YOUR CODE HERE\n","        # use your function n_gram_probability\n","        # vocab.itos contains a list of words to return probabilities for\n","        if len(text_prefix) < self.n - 1:\n","            # Use a uniform distribution over vocabulary\n","            return [1/self.vocab_size] * self.vocab_size\n","        \n","        # Extract the last n-1 words from text prefix \n","        prefix = text_prefix[-(self.n - 1):] if self.n > 1 else []\n","        \n","        # Calculate the probability for each word in vocabulary\n","        probabilities = []\n","        for word in vocab.itos:\n","            n_gram = prefix + [word]\n","            probabilities.append(self.n_gram_probability(n_gram))\n","\n","        \n","        return probabilities\n","        \n","        \n","    def perplexity(self, full_text):\n","        \"\"\" full_text is a list of string tokens\n","        return perplexity as a float \"\"\"\n","\n","        # YOUR CODE HERE\n","        # use your function n_gram_probability\n","        # This method should differ a bit from the example unigram model because \n","        # the first n-1 words of full_text must be handled as a special case.\n","        log_probabilities = []\n","        n_minus_1 = self.n - 1 \n","        \n","        if self.n > 1:\n","            for i in range(n_minus_1):\n","                log_probabilities.append(math.log(1/self.vocab_size, 2))\n","            \n","            for i in range(n_minus_1, len(full_text)):\n","                n_gram = full_text[i-n_minus_1:i+1]\n","                probability = self.n_gram_probability(n_gram)\n","                log_probabilities.append(math.log(probability, 2))\n","                if i % 5000 == 0 and i > 0:\n","                    print(f'Processed {i} tokens out of {len(full_text)}')\n","        else:\n","            for word in full_text:\n","                probability = self.n_gram_probability([word])\n","                log_probabilities.append(math.log(probability, 2))\n","                \n","\n","        \n","        return 2 ** -np.mean(log_probabilities)\n","        \n","\n","\n","unigram_model = NGramModel(train_text, 1)\n","# check_validity(unigram_model)\n","# print('unigram validation perplexity:', unigram_model.perplexity(validation_text)) # this should be the almost the same as our unigram model perplexity above\n","\n","# bigram_model = NGramModel(train_text, n=2)\n","# check_validity(bigram_model)\n","# print('bigram validation perplexity:', bigram_model.perplexity(validation_text))\n","\n","# trigram_model = NGramModel(train_text, n=3)\n","# check_validity(trigram_model)\n","# print('trigram validation perplexity:', trigram_model.perplexity(validation_text)) # this won't do very well...\n","\n","# save_truncated_distribution(bigram_model, 'bigram_predictions.npy') # this might take a few minutes"]},{"cell_type":"markdown","metadata":{"id":"TzRRLnk73-r9"},"source":["Please download `bigram_predictions.npy` once you finish this section so that you can submit it.\n","\n","In the block below, please report your bigram validation perplexity.  (We will use this to help us calibrate our scoring on the test set.)"]},{"cell_type":"markdown","metadata":{"id":"DEcUK27xVTcK"},"source":["<!-- Do not remove this comment, it is used by the autograder: RqYJKsoTS6 -->\n","\n","Bigram validation perplexity: ***504.42630378506976***"]},{"cell_type":"markdown","metadata":{"id":"qs6zgYw9VTx1"},"source":["We can also generate samples from the model to get an idea of how it is doing."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T00:55:56.438810Z","iopub.status.busy":"2024-09-08T00:55:56.437850Z","iopub.status.idle":"2024-09-08T00:55:58.185617Z","shell.execute_reply":"2024-09-08T00:55:58.184543Z","shell.execute_reply.started":"2024-09-08T00:55:56.438773Z"},"id":"m2V-qHxB4yhS","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<eos> <eos> = Football Union annexed prevails pédalier shop King Features petrol leading proponents fishery 1726 700 inventory appropriate for her \"\n"]}],"source":["print(generate_text(bigram_model))"]},{"cell_type":"markdown","metadata":{"id":"VsR8_Ch7AXAZ"},"source":["We now free up some RAM, **it is important to run the cell below, otherwise you may quite possibly run out of RAM in the runtime.**"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T01:05:56.248819Z","iopub.status.busy":"2024-09-08T01:05:56.247602Z","iopub.status.idle":"2024-09-08T01:05:56.391748Z","shell.execute_reply":"2024-09-08T01:05:56.390551Z","shell.execute_reply.started":"2024-09-08T01:05:56.248749Z"},"id":"EjKt1ncf_ypz","trusted":true},"outputs":[],"source":["# Free up some RAM. \n","del bigram_model\n","del trigram_model"]},{"cell_type":"markdown","metadata":{"id":"SWXNlsEKb3Mz"},"source":["This basic model works okay for bigrams, but a better strategy (especially for higher-order models) is to use backoff.  Implement backoff with absolute discounting.\n","$$P\\left(w_i|w_{i-n+1:i-1}\\right)=\\frac{max\\left\\{C(w_{i-n+1:i})-\\delta,0\\right\\}}{\\sum_{w' \\in V} C(w_{i-n+1:i-1}, w')} + \\alpha(w_{i-n+1:i-1}) P(w_i|w_{i-n+2:i-1})$$\n","\n","$$\\alpha\\left(w_{i-n+1:i-1}\\right)=\\frac{\\delta N_{1+}(w_{i-n+1:i-1})}{{\\sum_{w' \\in V} C(w_{i-n+1:i-1}, w')}}$$\n","where $V$ is the vocab and $N_{1+}$ is the number of words that appear after the previous $n-1$ words (the number of times the max will select something other than 0 in the first equation). $w_{i-n+1:i}$ denotes the $n$-gram starting at $w_{i-n+1}$ and ending at $w_i$, and $(w_{i-n+1:i-1}, w')$ denotes the n-gram containing the previous $n-1$ words followed by $w'$. If $\\sum_{w' \\in V} C(w_{i-n+1:i-1}, w')=0$, use the lower order model probability directly (the above equations would have a division by 0).\n","\n","We found a discount $\\delta$ of 0.9 to work well based on validation performance.  A trigram model with this discount value should get a validation perplexity below 275."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T19:33:08.800173Z","iopub.status.busy":"2024-09-11T19:33:08.799248Z","iopub.status.idle":"2024-09-11T19:33:08.804825Z","shell.execute_reply":"2024-09-11T19:33:08.803687Z","shell.execute_reply.started":"2024-09-11T19:33:08.800136Z"},"trusted":true},"outputs":[],"source":["from collections import Counter\n","from tqdm import tqdm  # Progress bar for large loops"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T19:36:45.527494Z","iopub.status.busy":"2024-09-11T19:36:45.527092Z","iopub.status.idle":"2024-09-11T21:53:57.471701Z","shell.execute_reply":"2024-09-11T21:53:57.470408Z","shell.execute_reply.started":"2024-09-11T19:36:45.527462Z"},"id":"BV4e4_mEc7VY","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training 2-gram model...\n","Precomputing distinct following word counts...\n"]},{"name":"stderr","output_type":"stream","text":["Precomputing following word counts: 100%|██████████| 33278/33278 [06:56<00:00, 79.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training 3-gram model...\n","Precomputing distinct following word counts...\n"]},{"name":"stderr","output_type":"stream","text":["Precomputing following word counts: 100%|██████████| 619592/619592 [2:10:00<00:00, 79.43it/s]  \n"]},{"name":"stdout","output_type":"stream","text":["Processed 5000 tokens out of 217646\n","Processed 10000 tokens out of 217646\n","Processed 15000 tokens out of 217646\n","Processed 20000 tokens out of 217646\n","Processed 25000 tokens out of 217646\n","Processed 30000 tokens out of 217646\n","Processed 35000 tokens out of 217646\n","Processed 40000 tokens out of 217646\n","Processed 45000 tokens out of 217646\n","Processed 50000 tokens out of 217646\n","Processed 55000 tokens out of 217646\n","Processed 60000 tokens out of 217646\n","Processed 65000 tokens out of 217646\n","Processed 70000 tokens out of 217646\n","Processed 75000 tokens out of 217646\n","Processed 80000 tokens out of 217646\n","Processed 85000 tokens out of 217646\n","Processed 90000 tokens out of 217646\n","Processed 95000 tokens out of 217646\n","Processed 100000 tokens out of 217646\n","Processed 105000 tokens out of 217646\n","Processed 110000 tokens out of 217646\n","Processed 115000 tokens out of 217646\n","Processed 120000 tokens out of 217646\n","Processed 125000 tokens out of 217646\n","Processed 130000 tokens out of 217646\n","Processed 135000 tokens out of 217646\n","Processed 140000 tokens out of 217646\n","Processed 145000 tokens out of 217646\n","Processed 150000 tokens out of 217646\n","Processed 155000 tokens out of 217646\n","Processed 160000 tokens out of 217646\n","Processed 165000 tokens out of 217646\n","Processed 170000 tokens out of 217646\n","Processed 175000 tokens out of 217646\n","Processed 180000 tokens out of 217646\n","Processed 185000 tokens out of 217646\n","Processed 190000 tokens out of 217646\n","Processed 195000 tokens out of 217646\n","Processed 200000 tokens out of 217646\n","Processed 205000 tokens out of 217646\n","Processed 210000 tokens out of 217646\n","Processed 215000 tokens out of 217646\n","trigram backoff validation perplexity: 271.12550435739735\n"]}],"source":["class DiscountBackoffModel(NGramModel):\n","    def __init__(self, train_text, lower_order_model, n=2, delta=0.9):\n","        super().__init__(train_text, n=n)\n","        self.lower_order_model = lower_order_model\n","        self.discount = delta\n","\n","        # YOUR CODE HERE\n","        \n","        print(f\"Training {n}-gram model...\")\n","\n","        self.n_gram_counts = Counter(tuple(train_text[i:i+n]) for i in range(len(train_text) - n + 1))\n","        self.n_minus_1_gram_counts = Counter(tuple(train_text[i:i+n-1]) for i in range(len(train_text) - n + 2))\n","        \n","        self.vocab = list(set(train_text))\n","        \n","        print(\"Precomputing distinct following word counts...\")\n","        # Precompute the number of distinct words that follow each (n-1)-gram\n","        self.following_words_count = {\n","            ngram: len([w for w in self.vocab if ngram + (w,) in self.n_gram_counts]) \n","            for ngram in tqdm(self.n_minus_1_gram_counts, desc=\"Precomputing following word counts\", position=0, leave=True)\n","        }\n","        \n","        \n","    def n_gram_probability(self, n_gram):\n","        assert len(n_gram) == self.n\n","\n","        # YOUR CODE HERE\n","        # back off to the lower_order model with n'=n-1 using its n_gram_probability function\n","        # Get the count of the full n-gram and the (n-1)-gram prefix\n","        \n","        n_minus_1_gram_tuple = tuple(n_gram[:-1])\n","        n_gram_tuple = tuple(n_gram)\n","\n","        # Get the count of the full n-gram and the (n-1)-gram prefix\n","        n_gram_count = self.n_gram_counts.get(n_gram_tuple, 0)\n","        n_minus_1_gram_count = self.n_minus_1_gram_counts.get(n_minus_1_gram_tuple, 0)\n","        \n","\n","        # Calculate alpha for discounting\n","        following_words_count = self.following_words_count.get(n_minus_1_gram_tuple, 0)\n","        alpha = (self.discount * following_words_count) / n_minus_1_gram_count if n_minus_1_gram_count > 0 else 1\n","\n","        # Compute the discounted probability or back off to the lower-order model\n","        return ((max(n_gram_count - self.discount, 0) / n_minus_1_gram_count) if n_minus_1_gram_count > 0 else 0) + alpha * self.lower_order_model.n_gram_probability(n_gram[1:])\n","    \n","\n","\n","bigram_backoff_model = DiscountBackoffModel(train_text, unigram_model, 2)\n","trigram_backoff_model = DiscountBackoffModel(train_text, bigram_backoff_model, 3)\n","check_validity(trigram_backoff_model)\n","print('trigram backoff validation perplexity:', trigram_backoff_model.perplexity(validation_text))"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T22:36:02.709099Z","iopub.status.busy":"2024-09-11T22:36:02.708418Z","iopub.status.idle":"2024-09-11T22:36:04.457852Z","shell.execute_reply":"2024-09-11T22:36:04.456985Z","shell.execute_reply.started":"2024-09-11T22:36:02.709059Z"},"trusted":true},"outputs":[],"source":["check_validity(trigram_backoff_model)\n","# save_truncated_distribution(bigram_backoff_model, 'bigram_backoff_model.npy') # this might take a few minutes\n","# save_truncated_distribution(trigram_backoff_model, 'trigram_backoff_model.npy') # this might take a few minutes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Release models we don't need any more. \n","del unigram_model\n","del bigram_backoff_model\n","del trigram_backoff_model"]},{"cell_type":"markdown","metadata":{"id":"LVrWYSMsBRSV"},"source":["Free up RAM. "]},{"cell_type":"markdown","metadata":{"id":"DPecL2jMXQ3y"},"source":["Fill in your trigram backoff perplexity here."]},{"cell_type":"markdown","metadata":{"id":"AIBVAMe0WV_1"},"source":["<!-- Do not remove this comment, it is used by the autograder: RqYJKsoTS6 -->\n","\n","Trigram backoff validation perplexity: ***271.13***\n","\n"]},{"cell_type":"markdown","metadata":{"id":"s3TFBf1CBiwp"},"source":["Free up RAM. "]},{"cell_type":"markdown","metadata":{"id":"e5Y0S6XbB1iZ"},"source":["### Neural N-gram Model\n","\n","In this section, you will implement a neural version of an n-gram model.  The model will use a simple feedforward neural network that takes the previous `n-1` words and outputs a distribution over the next word.\n","\n","You will use PyTorch to implement the model.  We've provided a little bit of code to help with the data loading using PyTorch's data loaders (https://pytorch.org/docs/stable/data.html)\n","\n","A model with the following architecture and hyperparameters should reach a validation perplexity below 226.\n","* embed the words with dimension 128, then flatten into a single embedding for $n-1$ words (with size $(n-1)*128$)\n","* run 2 hidden layers with 1024 hidden units, then project down to size 128 before the final layer (ie. 4 layers total). \n","* use weight tying for the embedding and final linear layer (this made a very large difference in our experiments); you can do this by creating the output layer with `nn.Linear`, then using `F.embedding` with the linear layer's `.weight` to embed the input\n","* rectified linear activation (ReLU) and dropout 0.1 after first 2 hidden layers. **Note: You will likely find a performance drop if you add a nonlinear activation function after the dimension reduction layer.**\n","* train for 10 epochs with the Adam optimizer (should take around 15-20 minutes)\n","* do early stopping based on validation set perplexity (see Project 0)\n","\n","\n","We encourage you to try other architectures and hyperparameters, and you will likely find some that work better than the ones listed above.  A proper implementation with these should be enough to receive full credit on the assignment, though."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-13T16:25:39.157912Z","iopub.status.busy":"2024-09-13T16:25:39.157345Z","iopub.status.idle":"2024-09-13T16:36:31.446528Z","shell.execute_reply":"2024-09-13T16:36:31.445485Z","shell.execute_reply.started":"2024-09-13T16:25:39.157879Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:114: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3005c14d034d4e8e9a5a48d3b39606fb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16318 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 5.862605750407347\n","Epoch 1, Validation Perplexity: 234.15843200683594\n","New best model saved with validation perplexity: 234.15843200683594\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c61c95784ec0466dbacac566bbe484eb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16318 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 5.370933127762684\n","Epoch 2, Validation Perplexity: 213.89324951171875\n","New best model saved with validation perplexity: 213.89324951171875\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"16b7bc0e1bf54563b220e3f5445e8bf5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16318 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 3, Loss: 5.199152794432473\n","Epoch 3, Validation Perplexity: 211.20938110351562\n","New best model saved with validation perplexity: 211.20938110351562\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2b40360a50b7446ea1e8e59dc48f54db","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16318 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 4, Loss: 5.093588398826343\n","Epoch 4, Validation Perplexity: 207.04798889160156\n","New best model saved with validation perplexity: 207.04798889160156\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"08440a7070744a689dc241fa86a0c90a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16318 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 5, Loss: 5.019865160991286\n","Epoch 5, Validation Perplexity: 208.89659118652344\n","No improvement in validation perplexity for 1 epoch(s).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9957407774384fd99c93afa19c186664","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16318 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 6, Loss: 4.963196056586648\n","Epoch 6, Validation Perplexity: 211.49327087402344\n","No improvement in validation perplexity for 2 epoch(s).\n","Early Stopping Trigger\n","neural trigram validation perplexity: 211.49327087402344\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:28: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["saved neural_trigram_predictions.npy\n"]}],"source":["def ids(tokens):\n","    return [vocab.stoi[t] for t in tokens]\n","\n","assert torch.cuda.is_available(), \"no GPU found; in Colab go to 'Edit->Notebook settings' and choose a GPU hardware accelerator; \\n in Kaggle go to 'Settings->Accelerator' and choose a GPU hardware accelerator\"\n","\n","class NeuralNgramDataset(torch.utils.data.Dataset):\n","    def __init__(self, text_token_ids, n):\n","        self.text_token_ids = text_token_ids\n","        self.n = n\n","\n","    def __len__(self):\n","        return len(self.text_token_ids)\n","\n","    def __getitem__(self, i):\n","        if i < self.n-1:\n","            prev_token_ids = [vocab.stoi['<eos>']] * (self.n-i-1) + self.text_token_ids[:i]\n","        else:\n","            prev_token_ids = self.text_token_ids[i-self.n+1:i]\n","\n","        assert len(prev_token_ids) == self.n-1\n","\n","        x = torch.tensor(prev_token_ids)\n","        y = torch.tensor(self.text_token_ids[i])\n","        return x, y\n","\n","class NeuralNGramNetwork(nn.Module):\n","    # a PyTorch Module that holds the neural network for your model\n","\n","    def __init__(self, n):\n","        super().__init__()\n","        self.n = n\n","\n","        # YOUR CODE HERE\n","        self.embedding_dim = 128\n","        self.hidden_dim = 1024\n","        self.vocab_size = vocab_size\n","        \n","        \n","        # Word embeddings\n","        self.embedding = nn.Embedding(vocab_size, self.embedding_dim)\n","        # Two hidden layers\n","        self.hidden_layer1 = nn.Linear((n-1) * self.embedding_dim, self.hidden_dim)\n","        self.hidden_layer2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n","        # Projection down to 128\n","        self.projection = nn.Linear(self.hidden_dim, self.embedding_dim)\n","        # Output layer with weight tying (bias = False to avoid breaking embedding-output tying)\n","        self.output_layer = nn.Linear(self.embedding_dim, vocab_size, bias=False)\n","        # Dropout\n","        self.dropout = nn.Dropout(0.1)\n","        \n","        \n","        \n","\n","\n","    def forward(self, x):\n","        # x is a tensor of inputs with shape (batch, n-1)\n","        # this function returns a tensor of log probabilities with shape (batch, vocab_size)\n","\n","        # YOUR CODE HERE\n","        # Naive Embedding \"embedded = self.embedding(x)\" provide worse result 271 Validation PPL\n","        \n","        # Step 1: Embed the input tokens using the output layer's weight\n","        embedded = F.embedding(x, self.output_layer.weight)  # Weight tying applied here\n","        # Step 2: Flatten the embeddings\n","        embedded = embedded.view(embedded.size(0), -1)  # (batch_size, (n-1) * embedding_dim)\n","        \n","        # Step 3: First hidden layer with ReLU and dropout\n","        h1 = F.relu(self.hidden_layer1(embedded))\n","        h1 = self.dropout(h1)\n","        \n","        h2 = F.relu(self.hidden_layer2(h1))\n","        h2 = self.dropout(h2)\n","        \n","        # Step 5: Project down to 128 dimensions\n","        projected = self.projection(h2)\n","        \n","        # Step 6: Output layer (using weight tying)\n","        output = F.linear(projected, self.output_layer.weight)\n","        \n","        return F.log_softmax(output, dim = -1)\n","        \n","\n","\n","class NeuralNGramModel:\n","    # a class that wraps NeuralNGramNetwork to handle training and evaluation\n","    # it's ok if this doesn't work for unigram modeling\n","    def __init__(self, n):\n","        self.n = n\n","        self.network = NeuralNGramNetwork(n).cuda()\n","        # YOUR CODE HERE\n","        self.criterion = nn.NLLLoss()\n","        self.optimizer = optim.Adam(self.network.parameters())\n","\n","    def train(self):\n","        dataset = NeuralNgramDataset(ids(train_text), self.n)\n","        train_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)\n","        # iterating over train_loader with a for loop will return a 2-tuple of batched tensors\n","        # the first tensor will be previous token ids with size (batch, n-1),\n","        # and the second will be the current token id with size (batch, )\n","        # you will need to move these tensors to GPU, e.g. by using the Tensor.cuda() function.\n","\n","        # this will take some time to run; use tqdm.tqdm_notebook to get a progress bar \n","        # (see Project 1a for example)\n","        \n","        best_validation_score = float('inf')\n","        patience = 1\n","        epochs_no_improve = 0\n","        best_model_path = 'best_model.pth'\n","\n","        # YOUR CODE HERE\n","        for epoch in range(10):\n","            self.network.train()\n","            total_loss = 0 \n","            for x_batch, y_batch in tqdm.tqdm_notebook(train_loader):\n","                x_batch, y_batch = x_batch.cuda(), y_batch.cuda()\n","                # Forward pass\n","                output = self.network(x_batch)\n","                # Compute loss\n","                loss = self.criterion(output, y_batch)\n","                # Backward pass and optimization\n","                self.optimizer.zero_grad()\n","                loss.backward()\n","                self.optimizer.step()\n","                \n","                total_loss += loss.item()\n","            \n","            print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')\n","            \n","            # Compute validation perplexity\n","            val_perplexity = self.perplexity(validation_text)\n","            print(f'Epoch {epoch+1}, Validation Perplexity: {val_perplexity}')\n","            \n","            # Early stopping check - stop when 3 no-improve on validation\n","            if val_perplexity < best_validation_score: \n","                best_validation_score = val_perplexity\n","                torch.save(self.network.state_dict(), best_model_path)\n","                print(f'New best model saved with validation perplexity: {val_perplexity}')\n","                epochs_no_improve = 0 \n","            else: \n","                epochs_no_improve += 1\n","                print(f'No improvement in validation perplexity for {epochs_no_improve} epoch(s).')\n","                if epochs_no_improve > patience:\n","                    print('Early Stopping Trigger')\n","                    break\n","                    \n","\n","\n","    def next_word_probabilities(self, text_prefix):\n","        # YOUR CODE HERE\n","        # don't forget self.network.eval()\n","        # you will need to convert text_prefix from strings to numbers with the `ids` function\n","        # if your `perplexity` function below is based on a NeuralNgramDataset DataLoader, you will need to use the same strategy for prefixes with less than n-1 tokens to pass the validity check\n","        #   the data loader appends extra \"<eos>\" (end of sentence) tokens to the start of the input so there are always enough to run the network\n","        self.network.eval()\n","        token_ids = ids(text_prefix)  # Convert text prefix to token ids\n","        \n","        # Pad with <eos> if the prefix length is less than n-1\n","        if len(token_ids) < self.n-1:\n","            token_ids = [vocab.stoi['<eos>']] * (self.n - 1 - len(token_ids)) + token_ids\n","        \n","        x = torch.tensor(token_ids[-(self.n-1):]).unsqueeze(0).cuda()\n","        with torch.no_grad():\n","            log_probs = self.network(x)\n","        \n","        return torch.exp(log_probs[0]).cpu().numpy()\n","\n","    def perplexity(self, text):\n","        # you may want to use a DataLoader here with a NeuralNgramDataset\n","        # don't forget self.network.eval()\n","\n","        # YOUR CODE HERE\n","        self.network.eval()\n","        dataset = NeuralNgramDataset(ids(text), self.n)\n","        data_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=False)\n","        \n","        total_log_prob = 0\n","        total_words = 0\n","        \n","        with torch.no_grad():\n","            for x_batch, y_batch in data_loader:\n","                x_batch, y_batch = x_batch.cuda(), y_batch.cuda()\n","                log_probs = self.network(x_batch)\n","                \n","                # Gather the log probabilities of the true words\n","                total_log_prob += F.nll_loss(log_probs, y_batch, reduction='sum').item()\n","                total_words += y_batch.size(0)\n","        \n","        perplexity = torch.exp(torch.tensor(total_log_prob) / total_words)\n","        return perplexity.item()\n","        \n","\n","\n","\n","neural_trigram_model = NeuralNGramModel(3)\n","# check_validity(neural_trigram_model)\n","neural_trigram_model.train()\n","print('neural trigram validation perplexity:', neural_trigram_model.perplexity(validation_text))\n","\n","save_truncated_distribution(neural_trigram_model, 'neural_trigram_predictions.npy', short=False)"]},{"cell_type":"markdown","metadata":{"id":"sm-xW4FGXYYi"},"source":["Fill in your neural trigram perplexity."]},{"cell_type":"markdown","metadata":{"id":"Q0cX0k2IW88k"},"source":["<!-- Do not remove this comment, it is used by the autograder: RqYJKsoTS6 -->\n","\n","Neural trigram validation perplexity: ***207***"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-13T16:46:13.605757Z","iopub.status.busy":"2024-09-13T16:46:13.605362Z","iopub.status.idle":"2024-09-13T16:46:13.752855Z","shell.execute_reply":"2024-09-13T16:46:13.751890Z","shell.execute_reply.started":"2024-09-13T16:46:13.605723Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<eos> <eos> = = = <eos> <eos> The church was a large number of aircraft and five . <eos> <eos> There was\n"]}],"source":["print(generate_text(neural_trigram_model))"]},{"cell_type":"markdown","metadata":{"id":"8t5PCZnkB1r5"},"source":["Free up RAM."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-13T16:46:21.750022Z","iopub.status.busy":"2024-09-13T16:46:21.749209Z","iopub.status.idle":"2024-09-13T16:46:21.754228Z","shell.execute_reply":"2024-09-13T16:46:21.753161Z","shell.execute_reply.started":"2024-09-13T16:46:21.749983Z"},"id":"x1yH0lGOB1-S","trusted":true},"outputs":[],"source":["# Delete model we don't need. \n","del neural_trigram_model"]},{"cell_type":"markdown","metadata":{"id":"qOp1Gb_0WjlE"},"source":["### LSTM Model\n","\n","For this stage of the project, you will implement an LSTM language model.\n","\n","For recurrent language modeling, the data batching strategy is a bit different from what is used in some other tasks.  Sentences are concatenated together so that one sentence starts right after the other, and an unfinished sentence will be continued in the next batch.  We'll use the `torchtext` library to manage this batching for you.  To properly deal with this input format, you should save the last state of the LSTM from a batch to feed in as the first state of the next batch.  When you save state across different batches, you should call `.detach()` on the state tensors before the next batch to tell PyTorch not to backpropagate gradients through the state into the batch you have already finished (which will cause a runtime error).\n","\n","We expect your model to reach a validation perplexity below 130.  The following architecture and hyperparameters should be sufficient to get there.\n","* 3 LSTM layers with 512 units\n","* dropout of 0.5 after each LSTM layer\n","* instead of projecting directly from the last LSTM output to the vocabulary size for softmax, project down to a smaller size first (e.g. 512->128->vocab_size). **NOTE: You may find that adding nonlinearities between these layers can hurt performance, try without first.**\n","* use the same weights for the embedding layer and the pre-softmax layer; dimension 128\n","* train with Adam (using default learning rates) for at least 20 epochs\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-09-13T16:50:33.471882Z","iopub.status.busy":"2024-09-13T16:50:33.471409Z","iopub.status.idle":"2024-09-13T17:11:05.791621Z","shell.execute_reply":"2024-09-13T17:11:05.790533Z","shell.execute_reply.started":"2024-09-13T16:50:33.471847Z"},"id":"0qOLXKKoc7If","outputId":"0516ca99-c13d-4968-a677-be29f7c2b839","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:92: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a0337f5960da483ab673ac88c455a713","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 6.823638892173767\n","Epoch 1, Validation Perplexity: 437.1129455566406\n","New best model saved with validation perplexity: 437.1129455566406\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9e89299e5865461c9e6a19b579b34434","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 6.0943540984509035\n","Epoch 2, Validation Perplexity: 324.0853271484375\n","New best model saved with validation perplexity: 324.0853271484375\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"470dc8f705844a5abf4ec3517e009848","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 3, Loss: 5.818457244891746\n","Epoch 3, Validation Perplexity: 268.1468200683594\n","New best model saved with validation perplexity: 268.1468200683594\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"14044b5a921441198d822ae0eeceb262","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 4, Loss: 5.620028014276542\n","Epoch 4, Validation Perplexity: 237.2594757080078\n","New best model saved with validation perplexity: 237.2594757080078\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb62351808794b58875fd70040bb1a07","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 5, Loss: 5.465574232737223\n","Epoch 5, Validation Perplexity: 215.70616149902344\n","New best model saved with validation perplexity: 215.70616149902344\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0affb086f2b44b42bfbedb10b8450821","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 6, Loss: 5.339913617395887\n","Epoch 6, Validation Perplexity: 200.96966552734375\n","New best model saved with validation perplexity: 200.96966552734375\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b1876c201ad84dc0b8151551da4c0054","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 7, Loss: 5.236591354538413\n","Epoch 7, Validation Perplexity: 189.84767150878906\n","New best model saved with validation perplexity: 189.84767150878906\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0a9ed0e15ee41febec44867fa0d4531","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 8, Loss: 5.147879929636039\n","Epoch 8, Validation Perplexity: 182.6044921875\n","New best model saved with validation perplexity: 182.6044921875\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a498b50cca6c4e03ada57a4d0afc2ac2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 9, Loss: 5.071899966165131\n","Epoch 9, Validation Perplexity: 175.56724548339844\n","New best model saved with validation perplexity: 175.56724548339844\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"70e1e8f006764f20a85aa988f55e208b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 10, Loss: 5.003757230440775\n","Epoch 10, Validation Perplexity: 170.86135864257812\n","New best model saved with validation perplexity: 170.86135864257812\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"55a5d55d7202478d9a8a71b5259b9e33","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 11, Loss: 4.943912158292883\n","Epoch 11, Validation Perplexity: 166.5323486328125\n","New best model saved with validation perplexity: 166.5323486328125\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a5199feed194449ea933afa94a273b4c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 12, Loss: 4.889673208722881\n","Epoch 12, Validation Perplexity: 164.2957000732422\n","New best model saved with validation perplexity: 164.2957000732422\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6e1a097d06c4a0583e5b69084b5d05c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 13, Loss: 4.839953689949185\n","Epoch 13, Validation Perplexity: 162.62049865722656\n","New best model saved with validation perplexity: 162.62049865722656\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2ce235927dd54867a9fd6e1b8c43660f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 14, Loss: 4.794508008863412\n","Epoch 14, Validation Perplexity: 159.71224975585938\n","New best model saved with validation perplexity: 159.71224975585938\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"02434a4421b84ad7bb3978f0bdff6298","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 15, Loss: 4.751493612925212\n","Epoch 15, Validation Perplexity: 158.36021423339844\n","New best model saved with validation perplexity: 158.36021423339844\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e4398a05e7db437a926bc2c16aed3c7c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 16, Loss: 4.7129925928863825\n","Epoch 16, Validation Perplexity: 158.4795684814453\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ad90eacf7474fd39e216a10ef26f290","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 17, Loss: 4.676219066451577\n","Epoch 17, Validation Perplexity: 157.0289764404297\n","New best model saved with validation perplexity: 157.0289764404297\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"483b422a5a8b4156810a69e5dcd08113","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 18, Loss: 4.641533553366568\n","Epoch 18, Validation Perplexity: 157.2941131591797\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2adf9bc4bc04765b9891a097c772e4c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 19, Loss: 4.609416576927783\n","Epoch 19, Validation Perplexity: 156.60104370117188\n","New best model saved with validation perplexity: 156.60104370117188\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce98c6e6d1004e669a48f5c46ba3528c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 20, Loss: 4.578115526367636\n","Epoch 20, Validation Perplexity: 157.7437286376953\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f3f489618b3c4c948100d2c77b978f65","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 21, Loss: 4.548326000980302\n","Epoch 21, Validation Perplexity: 156.92739868164062\n","Early stopping triggered.\n","lstm validation perplexity: 156.5919952392578\n"]}],"source":["class LSTMNetwork(nn.Module):\n","    # a PyTorch Module that holds the neural network for your model\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","        # YOUR CODE HERE\n","        self.embedding_dim = 128\n","        self.hidden_dim = 512\n","        self.lstm_layers = 3\n","        self.vocab_size = vocab_size\n","        \n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n","        \n","        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, self.lstm_layers, dropout=0.5)\n","\n","        self.projection1 = nn.Linear(self.hidden_dim, self.embedding_dim)\n","        # Output layer: bias=False for weight tying\n","        self.projection2 = nn.Linear(self.embedding_dim, self.vocab_size, bias=False)\n","        \n","        # Weight tying\n","        self.projection2.weight = self.embedding.weight\n","\n","\n","    def forward(self, x, state):\n","        \"\"\"Compute the output of the network.\n","        \n","        Note: In the Pytorch LSTM tutorial, the state variable is named \"hidden\":\n","        https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n","\n","        The torch.nn.LSTM documentation is quite helpful:\n","        https://pytorch.org/docs/stable/nn.html#lstm\n","    \n","        x - a tensor of int64 inputs with shape (seq_len, batch)\n","        state - a tuple of two tensors with shape (num_layers, batch, hidden_size)\n","                representing the hidden state and cell state of the of the LSTM.\n","        returns a tuple with two elements:\n","          - a tensor of log probabilities with shape (seq_len, batch, vocab_size)\n","          - a state tuple returned by applying the LSTM.\n","        \"\"\"\n","\n","        # Note that the nn.LSTM module expects inputs with the sequence \n","        # dimension before the batch by default.\n","        # In this case the dimensions are already in the right order, \n","        # but watch out for this since sometimes people put the batch first\n","\n","        # YOUR CODE HERE\n","        embedded = self.embedding(x)  # Use weight tying\n","        lstm_output, state = self.lstm(embedded, state) # LSTM output (seq_len, batch, hidden_dim)\n","        \n","        # Projection to 128 then Vocab Size\n","        projected_output = self.projection1(lstm_output)\n","        output = self.projection2(projected_output)\n","        \n","        \n","        return F.log_softmax(output,dim=-1), state\n","        \n","\n","\n","class LSTMModel:\n","    \"A class that wraps LSTMNetwork to handle training and evaluation.\"\n","\n","    def __init__(self):\n","        self.network = LSTMNetwork().cuda()\n","        # YOUR CODE HERE\n","        self.criterion = nn.NLLLoss()\n","        self.optimizer = optim.Adam(self.network.parameters())\n","        self.patience = 2  # Early stopping patience\n","        self.best_model_path = 'best_lstm_model.pth'\n","\n","    def train(self):\n","        train_iterator = data.BPTTIterator(train_dataset, batch_size=64, \n","                                                     bptt_len=32, device='cuda')\n","        # Iterate over train_iterator with a for loop to get batches\n","        # each batch object has a .text and .target attribute with\n","        # token id tensors for the input and output respectively.\n","\n","        # The initial state passed into the LSTM should be set to zero.\n","\n","        # YOUR CODE HERE\n","        best_val_loss = float('inf')\n","        epochs_no_improve = 0\n","        \n","        for epoch in range(30):\n","            total_loss = 0\n","            self.network.train()\n","\n","            # Initialize the hidden and cell states to zeros at the start of the epoch\n","            state = (torch.zeros(self.network.lstm_layers, 64, self.network.hidden_dim).cuda(),\n","                     torch.zeros(self.network.lstm_layers, 64, self.network.hidden_dim).cuda())\n","            \n","            for batch in tqdm.tqdm_notebook(train_iterator):\n","                inputs, targets = batch.text, batch.target\n","                \n","                state = (state[0].detach(), state[1].detach()) # Detach state for the next batch\n","                \n","                self.optimizer.zero_grad()\n","                outputs, states = self.network(inputs, state)\n","                loss = self.criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n","                loss.backward()\n","                self.optimizer.step()\n","                total_loss += loss.item()\n","                \n","            print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_iterator)}')\n","        \n","            # Validate the model and apply early stopping\n","            val_perplexity = self.dataset_perplexity(validation_dataset)\n","            print(f'Epoch {epoch + 1}, Validation Perplexity: {val_perplexity}')\n","            \n","            # Check for improvement in validation perplexity\n","            if val_perplexity < best_val_loss:\n","                best_val_loss = val_perplexity\n","                epochs_no_improve = 0\n","                torch.save(self.network.state_dict(), self.best_model_path)\n","                print(f'New best model saved with validation perplexity: {val_perplexity}')\n","            else:\n","                epochs_no_improve += 1\n","\n","            # Early stopping if no improvement for patience epochs\n","            if epochs_no_improve >= self.patience:\n","                print(\"Early stopping triggered.\")\n","                break\n","\n","\n","    def next_word_probabilities(self, text_prefix):\n","        \"Return a list of probabilities for each word in the vocabulary.\"\n","\n","        prefix_token_tensor = torch.tensor(ids(text_prefix), device='cuda').view(-1, 1)\n","        \n","        # YOUR CODE HERE\n","        self.network.eval()\n","        state = (torch.zeros(self.network.lstm_layers, 1, self.network.hidden_dim).cuda(),\n","                 torch.zeros(self.network.lstm_layers, 1, self.network.hidden_dim).cuda())\n","        \n","        with torch.no_grad():\n","            log_probs, new_state = self.network(prefix_token_tensor, state)\n","        \n","        return torch.exp(log_probs[-1, 0]).cpu().numpy()\n","        \n","        \n","\n","    def dataset_perplexity(self, torchtext_dataset):\n","        \"Return perplexity as a float.\"\n","        # Your code should be very similar to next_word_probabilities, but\n","        # run in a loop over batches. Use torch.no_grad() for extra speed.\n","\n","        iterator = data.BPTTIterator(torchtext_dataset, batch_size=64, bptt_len=32, device='cuda')\n","\n","        # YOUR CODE HERE\n","        total_loss, total_words = 0, 0 \n","        state = (torch.zeros(self.network.lstm_layers, 64, self.network.hidden_dim).cuda(),\n","                 torch.zeros(self.network.lstm_layers, 64, self.network.hidden_dim).cuda())\n","        \n","        with torch.no_grad():\n","            for batch in iterator:\n","                inputs, targets = batch.text, batch.target\n","                state = (state[0].detach(), state[1].detach())\n","                outputs, state = self.network(inputs, state)\n","                \n","                loss = F.nll_loss(outputs.view(-1, outputs.size(-1)), targets.view(-1), reduction = 'sum')\n","                \n","                total_loss += loss.item()\n","                total_words += targets.numel()\n","        return torch.exp(torch.tensor(total_loss / total_words)).item()\n","        \n","\n","lstm_model = LSTMModel()\n","lstm_model.train()\n","\n","print('lstm validation perplexity:', lstm_model.dataset_perplexity(validation_dataset))"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-09-13T17:11:08.855240Z","iopub.status.busy":"2024-09-13T17:11:08.854853Z","iopub.status.idle":"2024-09-13T17:11:20.826232Z","shell.execute_reply":"2024-09-13T17:11:20.825246Z","shell.execute_reply.started":"2024-09-13T17:11:08.855210Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:28: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["saved lstm_predictions.npy\n"]}],"source":["save_truncated_distribution(lstm_model, 'lstm_predictions.npy', short=False)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-09-13T17:25:03.783254Z","iopub.status.busy":"2024-09-13T17:25:03.782865Z","iopub.status.idle":"2024-09-13T17:25:03.960574Z","shell.execute_reply":"2024-09-13T17:25:03.959580Z","shell.execute_reply.started":"2024-09-13T17:25:03.783220Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<eos> <eos> = = Major @-@ life Management = = <eos> <eos> Weir was commissioned to fragment for the outbreak of World\n"]}],"source":["print(generate_text(lstm_model))"]},{"cell_type":"markdown","metadata":{"id":"7pGhdPQqHx9v"},"source":["<!-- Do not remove this comment, it is used by the autograder: RqYJKsoTS6 -->\n","\n","Fill in your LSTM perplexity. \n","\n","LSTM validation perplexity: ***157***"]},{"cell_type":"markdown","metadata":{"id":"kLoiXBWMaSPc"},"source":["# Experimentation: 1-Page Report\n","\n","Now it's time for you to experiment.  Try to reach a validation perplexity below 120. You may either modify the LSTM class above, or copy it down to the code cell below and modify it there. Just **be sure to run code cell below to generate results with your improved LSTM**.  \n","\n","It is okay if the bulk of your improvements are due to hyperparameter tuning (such as changing number or sizes of layers), but implement at least one more substantial change to the model.  Here are some ideas (several of which come from https://arxiv.org/pdf/1708.02182.pdf):\n","* activation regularization - add a l2 regularization penalty on the activation of the LSTM output (standard l2 regularization is on the weights)\n","* weight-drop regularization - apply dropout to the weight matrices instead of activations\n","* learning rate scheduling - decrease the learning rate during training\n","* embedding dropout - zero out the entire embedding for a random set of words in the embedding matrix\n","* ensembling - average the predictions of several models trained with different initialization random seeds\n","* temporal activation regularization - add l2 regularization on the difference between the LSTM output activations at adjacent timesteps\n","\n","You may notice that most of these suggestions are regularization techniques.  This dataset is considered fairly small, so regularization is one of the best ways to improve performance.\n","\n","For this section, you will submit a write-up describing the extensions and/or modifications that you tried.  Your write-up should be **1-page maximum** in length and should be submitted in PDF format.  You may use any editor you like, but we recommend using LaTeX and working in an environment like Overleaf.\n","For full credit, your write-up should include:\n","1.   A concise and precise description of the extension that you tried.\n","2.   A motivation for why you believed this approach might improve your model.\n","3.   A discussion of whether the extension was effective and/or an analysis of the results.  This will generally involve some combination of tables, learning curves, etc.\n","4.   A bottom-line summary of your results comparing validation perplexities of your improvement to the original LSTM.\n","The purpose of this exercise is to experiment, so feel free to try/ablate multiple of the suggestions above as well as any others you come up with!\n","When you submit the file, please name it `report.pdf`.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"i4fxj-BQTDgU"},"source":["Run the cell below in order to train your improved LSTM and evaluate it.  "]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-09-13T18:21:43.682407Z","iopub.status.busy":"2024-09-13T18:21:43.681809Z","iopub.status.idle":"2024-09-13T18:48:56.814008Z","shell.execute_reply":"2024-09-13T18:48:56.812988Z","shell.execute_reply.started":"2024-09-13T18:21:43.682365Z"},"id":"Z_244hhNP9PO","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:116: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"191e4cff0acc4f458a4e8d9090cad46a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 6.809304465032091\n","Epoch 1, Validation Perplexity: 431.561279296875\n","New best model saved with validation perplexity: 431.561279296875\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"90ae97ed7dbe4813afca0553434f741e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 6.074475097188762\n","Epoch 2, Validation Perplexity: 319.9286193847656\n","New best model saved with validation perplexity: 319.9286193847656\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"536b7b6eb5cd4895a7635d0fcaffe0c0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 3, Loss: 5.799917399182039\n","Epoch 3, Validation Perplexity: 266.8653869628906\n","New best model saved with validation perplexity: 266.8653869628906\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"03075906cd6a4798821e2388e8586db4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 4, Loss: 5.603944595187318\n","Epoch 4, Validation Perplexity: 234.23101806640625\n","New best model saved with validation perplexity: 234.23101806640625\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"64e956b949ea465589aaadd9596d7070","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 5, Loss: 5.450821961608588\n","Epoch 5, Validation Perplexity: 212.7420196533203\n","New best model saved with validation perplexity: 212.7420196533203\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca0097962709471e883b978f964f5e97","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 6, Loss: 5.327732241854948\n","Epoch 6, Validation Perplexity: 196.43463134765625\n","New best model saved with validation perplexity: 196.43463134765625\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d6c162b71cba4718bed0a816525c7a58","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 7, Loss: 5.227847313413433\n","Epoch 7, Validation Perplexity: 185.91220092773438\n","New best model saved with validation perplexity: 185.91220092773438\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"285dc8a90163430e94afa6e824ee3898","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 8, Loss: 5.142344941344915\n","Epoch 8, Validation Perplexity: 177.3015899658203\n","New best model saved with validation perplexity: 177.3015899658203\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6154fe241113443db88e0c093919d4c6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 9, Loss: 5.069100797410105\n","Epoch 9, Validation Perplexity: 170.15765380859375\n","New best model saved with validation perplexity: 170.15765380859375\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a5dcdce8cc64afcb7db5fbe37187fdf","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 10, Loss: 5.005309290979422\n","Epoch 10, Validation Perplexity: 165.6397247314453\n","New best model saved with validation perplexity: 165.6397247314453\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aa6aca6b5b3c4c3f94d38b5bc1ce8222","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 11, Loss: 4.948205143797631\n","Epoch 11, Validation Perplexity: 161.01425170898438\n","New best model saved with validation perplexity: 161.01425170898438\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5972df69096c4e05998cccd4f3ccfdb4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 12, Loss: 4.897338754055546\n","Epoch 12, Validation Perplexity: 157.48060607910156\n","New best model saved with validation perplexity: 157.48060607910156\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9038c3c89734477c8edb9cc2b1562cb3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 13, Loss: 4.850997534920188\n","Epoch 13, Validation Perplexity: 154.15867614746094\n","New best model saved with validation perplexity: 154.15867614746094\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"32adb0cd31e749b28ba287b7a863a3e1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 14, Loss: 4.809218663795321\n","Epoch 14, Validation Perplexity: 152.0447235107422\n","New best model saved with validation perplexity: 152.0447235107422\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c8d4c90df4df45e0b3e42f4796154a33","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 15, Loss: 4.769125881382063\n","Epoch 15, Validation Perplexity: 150.25213623046875\n","New best model saved with validation perplexity: 150.25213623046875\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"feec1ef50dbe4f399180395b4f1b7bb4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 16, Loss: 4.7331113455342315\n","Epoch 16, Validation Perplexity: 148.34962463378906\n","New best model saved with validation perplexity: 148.34962463378906\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"298cfcfecabc4b66b37860831ffbad32","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 17, Loss: 4.700017370897181\n","Epoch 17, Validation Perplexity: 147.46815490722656\n","New best model saved with validation perplexity: 147.46815490722656\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"55764a16f62647b69b6fc1b04dba860a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 18, Loss: 4.668002068295198\n","Epoch 18, Validation Perplexity: 145.91783142089844\n","New best model saved with validation perplexity: 145.91783142089844\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9fad7865c403475490794dbfa2e0367b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 19, Loss: 4.638793729333317\n","Epoch 19, Validation Perplexity: 144.6492462158203\n","New best model saved with validation perplexity: 144.6492462158203\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d8d0d11e3a654122ab1ea30eae22b22a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 20, Loss: 4.611190943156972\n","Epoch 20, Validation Perplexity: 144.38958740234375\n","New best model saved with validation perplexity: 144.38958740234375\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ffa84babeedb45f896d34cbfa3840413","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 21, Loss: 4.584291686263739\n","Epoch 21, Validation Perplexity: 144.14564514160156\n","New best model saved with validation perplexity: 144.14564514160156\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6a51142d4d0f4e758fb04a497fa65914","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 22, Loss: 4.55896888480467\n","Epoch 22, Validation Perplexity: 143.5348663330078\n","New best model saved with validation perplexity: 143.5348663330078\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a72fa1dca44433c9fef9f77ddcf2ac9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 23, Loss: 4.53425007614435\n","Epoch 23, Validation Perplexity: 142.9739227294922\n","New best model saved with validation perplexity: 142.9739227294922\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"23e389623ae04dbaa035f5e9024077c9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 24, Loss: 4.510986056514815\n","Epoch 24, Validation Perplexity: 142.56272888183594\n","New best model saved with validation perplexity: 142.56272888183594\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9e169e4d5a484789866484d27c218eda","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 25, Loss: 4.489686052471984\n","Epoch 25, Validation Perplexity: 142.32106018066406\n","New best model saved with validation perplexity: 142.32106018066406\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8367fe3010f54e2aa7575ea21c0844ad","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 26, Loss: 4.467238087747611\n","Epoch 26, Validation Perplexity: 141.93104553222656\n","New best model saved with validation perplexity: 141.93104553222656\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"47e3c4316b75496584ebf8106a208fc4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 27, Loss: 4.447107078514847\n","Epoch 27, Validation Perplexity: 142.32452392578125\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"538c2c917456409587d539f7061b1d08","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1020 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 28, Loss: 4.427009384772357\n","Epoch 28, Validation Perplexity: 142.42384338378906\n","Early stopping triggered.\n","lstm validation perplexity: 142.0345001220703\n"]}],"source":["class WeightDropLSTM(nn.LSTM):\n","    \"\"\"LSTM with weight drop applied to hidden-to-hidden weights\"\"\"\n","    def __init__(self, *args, weight_dropout=0.5, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.weight_dropout = weight_dropout\n","        self._setup_weights()\n","\n","    def _setup_weights(self):\n","        # Apply dropout to the recurrent weights (weight_hh_l0)\n","        self.weight_hh_l0_raw = nn.Parameter(self.weight_hh_l0.data.clone())\n","        self.weight_hh_l0.requires_grad = False\n","\n","    def _setweights(self):\n","        self.weight_hh_l0.data.copy_(self.weight_hh_l0_raw.data)\n","        mask = torch.ones(self.weight_hh_l0.data.size(0), self.weight_hh_l0.data.size(1), device=self.weight_hh_l0.device)\n","        mask = torch.nn.functional.dropout(mask, self.weight_dropout, training=self.training)\n","        self.weight_hh_l0.data.mul_(mask)\n","\n","    def forward(self, *args, **kwargs):\n","        self._setweights()\n","        return super().forward(*args, **kwargs)\n","    \n","class LSTMNetwork(nn.Module):\n","    # a PyTorch Module that holds the neural network for your model\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","        # YOUR CODE HERE\n","        self.embedding_dim = 128\n","        self.hidden_dim = 512\n","        self.lstm_layers = 3\n","        self.vocab_size = vocab_size\n","        \n","        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n","        \n","        # self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, self.lstm_layers, dropout=0.5)\n","        self.lstm = WeightDropLSTM(self.embedding_dim, self.hidden_dim, self.lstm_layers, dropout=0.5)\n","\n","\n","        self.projection1 = nn.Linear(self.hidden_dim, self.embedding_dim)\n","        # Output layer: bias=False for weight tying\n","        self.projection2 = nn.Linear(self.embedding_dim, self.vocab_size, bias=False)\n","        \n","        # Weight tying\n","        self.projection2.weight = self.embedding.weight\n","\n","\n","    def forward(self, x, state):\n","        \"\"\"Compute the output of the network.\n","        \n","        Note: In the Pytorch LSTM tutorial, the state variable is named \"hidden\":\n","        https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n","\n","        The torch.nn.LSTM documentation is quite helpful:\n","        https://pytorch.org/docs/stable/nn.html#lstm\n","    \n","        x - a tensor of int64 inputs with shape (seq_len, batch)\n","        state - a tuple of two tensors with shape (num_layers, batch, hidden_size)\n","                representing the hidden state and cell state of the of the LSTM.\n","        returns a tuple with two elements:\n","          - a tensor of log probabilities with shape (seq_len, batch, vocab_size)\n","          - a state tuple returned by applying the LSTM.\n","        \"\"\"\n","\n","        # Note that the nn.LSTM module expects inputs with the sequence \n","        # dimension before the batch by default.\n","        # In this case the dimensions are already in the right order, \n","        # but watch out for this since sometimes people put the batch first\n","\n","        # YOUR CODE HERE\n","        embedded = self.embedding(x)  # Use weight tying\n","        lstm_output, state = self.lstm(embedded, state) # LSTM output (seq_len, batch, hidden_dim)\n","        \n","        # Projection to 128 then Vocab Size\n","        projected_output = self.projection1(lstm_output)\n","        output = self.projection2(projected_output)\n","        \n","        \n","        return F.log_softmax(output,dim=-1), state\n","        \n","\n","\n","class LSTMModel:\n","    \"A class that wraps LSTMNetwork to handle training and evaluation.\"\n","\n","    def __init__(self):\n","        self.network = LSTMNetwork().cuda()\n","        # YOUR CODE HERE\n","        self.criterion = nn.NLLLoss()\n","        self.optimizer = optim.Adam(self.network.parameters())\n","        self.patience = 2  # Early stopping patience\n","        self.best_model_path = 'best_lstm_model.pth'\n","\n","    def train(self):\n","        train_iterator = data.BPTTIterator(train_dataset, batch_size=64, \n","                                                     bptt_len=32, device='cuda')\n","        # Iterate over train_iterator with a for loop to get batches\n","        # each batch object has a .text and .target attribute with\n","        # token id tensors for the input and output respectively.\n","\n","        # The initial state passed into the LSTM should be set to zero.\n","\n","        # YOUR CODE HERE\n","        best_val_loss = float('inf')\n","        epochs_no_improve = 0\n","        \n","        for epoch in range(30):\n","            total_loss = 0\n","            self.network.train()\n","\n","            # Initialize the hidden and cell states to zeros at the start of the epoch\n","            state = (torch.zeros(self.network.lstm_layers, 64, self.network.hidden_dim).cuda(),\n","                     torch.zeros(self.network.lstm_layers, 64, self.network.hidden_dim).cuda())\n","            \n","            for batch in tqdm.tqdm_notebook(train_iterator):\n","                inputs, targets = batch.text, batch.target\n","                \n","                state = (state[0].detach(), state[1].detach()) # Detach state for the next batch\n","                \n","                self.optimizer.zero_grad()\n","                outputs, states = self.network(inputs, state)\n","                loss = self.criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n","                loss.backward()\n","                self.optimizer.step()\n","                total_loss += loss.item()\n","                \n","            print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_iterator)}')\n","        \n","            # Validate the model and apply early stopping\n","            val_perplexity = self.dataset_perplexity(validation_dataset)\n","            print(f'Epoch {epoch + 1}, Validation Perplexity: {val_perplexity}')\n","            \n","            # Check for improvement in validation perplexity\n","            if val_perplexity < best_val_loss:\n","                best_val_loss = val_perplexity\n","                epochs_no_improve = 0\n","                torch.save(self.network.state_dict(), self.best_model_path)\n","                print(f'New best model saved with validation perplexity: {val_perplexity}')\n","            else:\n","                epochs_no_improve += 1\n","\n","            # Early stopping if no improvement for patience epochs\n","            if epochs_no_improve >= self.patience:\n","                print(\"Early stopping triggered.\")\n","                break\n","\n","\n","    def next_word_probabilities(self, text_prefix):\n","        \"Return a list of probabilities for each word in the vocabulary.\"\n","\n","        prefix_token_tensor = torch.tensor(ids(text_prefix), device='cuda').view(-1, 1)\n","        \n","        # YOUR CODE HERE\n","        self.network.eval()\n","        state = (torch.zeros(self.network.lstm_layers, 1, self.network.hidden_dim).cuda(),\n","                 torch.zeros(self.network.lstm_layers, 1, self.network.hidden_dim).cuda())\n","        \n","        with torch.no_grad():\n","            log_probs, new_state = self.network(prefix_token_tensor, state)\n","        \n","        return torch.exp(log_probs[-1, 0]).cpu().numpy()\n","        \n","        \n","\n","    def dataset_perplexity(self, torchtext_dataset):\n","        \"Return perplexity as a float.\"\n","        # Your code should be very similar to next_word_probabilities, but\n","        # run in a loop over batches. Use torch.no_grad() for extra speed.\n","\n","        iterator = data.BPTTIterator(torchtext_dataset, batch_size=64, bptt_len=32, device='cuda')\n","\n","        # YOUR CODE HERE\n","        total_loss, total_words = 0, 0 \n","        state = (torch.zeros(self.network.lstm_layers, 64, self.network.hidden_dim).cuda(),\n","                 torch.zeros(self.network.lstm_layers, 64, self.network.hidden_dim).cuda())\n","        \n","        with torch.no_grad():\n","            for batch in iterator:\n","                inputs, targets = batch.text, batch.target\n","                state = (state[0].detach(), state[1].detach())\n","                outputs, state = self.network(inputs, state)\n","                \n","                loss = F.nll_loss(outputs.view(-1, outputs.size(-1)), targets.view(-1), reduction = 'sum')\n","                \n","                total_loss += loss.item()\n","                total_words += targets.numel()\n","        return torch.exp(torch.tensor(total_loss / total_words)).item()\n","        \n","\n","lstm_model = LSTMModel()\n","lstm_model.train()\n","\n","print('lstm validation perplexity:', lstm_model.dataset_perplexity(validation_dataset))"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-09-13T18:54:56.263059Z","iopub.status.busy":"2024-09-13T18:54:56.262520Z","iopub.status.idle":"2024-09-13T18:55:08.823632Z","shell.execute_reply":"2024-09-13T18:55:08.822588Z","shell.execute_reply.started":"2024-09-13T18:54:56.263024Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:28: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["saved lstm_predictions_WeightDrop.npy\n","<eos> <eos> Originally Diplocystaceae was all Greek , particularly classified as \" <unk> \" articulated from other languages . <eos> <unk> <unk>\n"]}],"source":["save_truncated_distribution(lstm_model, 'lstm_predictions_WeightDrop.npy', short=False)\n","print(generate_text(lstm_model))"]},{"cell_type":"markdown","metadata":{"id":"BHTOfrCG8CRF"},"source":["### Submission\n","\n","Upload a submission with the following files to Gradescope:\n","* hw1b.ipynb (rename to match this exactly)\n","* lstm_predictions.npy (this should also include all improvements from your exploration)\n","* neural_trigram_predictions.npy\n","* bigram_predictions.npy\n","* report.pdf\n","\n","You can upload files individually or as part of a zip file, but if using a zip file be sure you are zipping the files directly and not a folder that contains them.\n","\n","Be sure to check the output of the autograder after it runs.  It should confirm that no files are missing and that the output files have the correct format.  Note that the test set perplexities shown by the autograder are on a completely different scale from your validation set perplexities due to truncating the distribution and selecting different text.  Don't worry if the values seem much worse."]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5614701,"sourceId":9276871,"sourceType":"datasetVersion"}],"dockerImageVersionId":30370,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
